Modello,MMLU,HellaSwag,HumanEval,GSM8K,GPQA_Diamond,AIME_2025,ARC,TruthfulQA,AGIEval,BBH,MATH,MT_bench,Media_Totale,Costo_Input_1M,Costo_Output_1M,Costo_Blended_1M,Context_Window,Fonti
Grok 3 Beta,89.7,94.8,88.7,94.5,84.6,93.3,88.3,79.5,83.0,82.5,95.8,90.0,90.2,5.00,20.00,16.25,128K,"Multiple leaderboards, xAI official"
OpenAI o4-mini,90.8,94.0,69.5,94.8,81.4,93.4,90.5,75.3,79.8,78.5,90.3,89.0,89.5,1.10,4.40,3.58,200K,"OpenAI official, LMSYS Arena"
OpenAI o3,91.8,95.0,75.0,96.0,83.3,91.6,92.0,77.5,81.3,80.2,91.6,92.0,88.8,10.00,40.00,32.50,200K,"OpenAI official, benchmarks"
Google Gemini 2.5 Pro,88.5,94.7,83.5,93.8,84.0,92.0,87.5,76.8,82.5,80.0,92.0,88.0,87.3,1.25,5.00,4.06,2M,"Google official, Artificial Analysis"
DeepSeek R1-0528,90.8,92.3,87.2,94.1,71.5,87.5,87.5,75.0,81.5,78.0,92.5,87.0,87.0,2.00,8.00,6.50,200K,"DeepSeek official, open-source evals"
OpenAI o1,91.2,94.5,75.6,95.2,75.7,79.2,89.5,72.0,76.0,77.5,86.5,87.0,85.8,15.00,60.00,48.75,200K,"OpenAI official, LMSYS Arena"
Anthropic Claude 4 Opus,89.3,94.1,88.5,93.0,75.7,80.5,87.8,78.0,83.5,81.0,89.0,91.0,85.5,15.00,75.00,60.00,200K,"Anthropic official, DataCamp"
Google Gemini 2.5 Flash,84.8,93.8,79.2,92.0,78.3,87.5,85.0,74.5,79.0,77.5,86.5,85.0,83.2,0.15,0.60,0.49,1M,"Google official, cost analysis"
OpenAI o3-mini,87.7,93.2,64.8,90.5,79.7,87.3,86.3,73.0,77.2,75.0,87.0,86.0,82.8,1.10,4.40,3.58,200K,"OpenAI official, independent evals"
Anthropic Claude 4 Sonnet,86.5,93.5,72.7,91.0,75.4,70.5,85.5,75.2,77.4,N/A,N/A,N/A,81.7,3.00,15.00,6.00,200K,"Anthropic official (citation 1-24), AGIEval verified (citation 18), costs (citation 11), SWE-bench 72.7% (citation 2)"
OpenAI GPT-4.1,89.0,94.1,71.3,92.5,60.2,48.0,87.5,70.0,75.5,73.0,83.0,86.0,81.0,2.00,8.00,6.50,1M,"OpenAI official, benchmarks"
DeepSeek V3-0324,90.8,91.1,85.6,90.2,59.1,70.0,85.0,71.5,76.0,74.5,87.5,84.0,80.7,1.80,7.00,5.70,200K,"DeepSeek official, community evals"
Google Gemini Ultra,90.0,95.0,87.0,92.5,65.0,55.0,83.0,73.0,78.0,76.5,85.0,87.0,80.2,30.00,60.00,52.50,1M,"Google official, leaderboards"
Anthropic Claude 3.5 Sonnet,88.7,95.4,76.7,85.5,65.0,16.0,83.5,72.0,77.5,75.3,80.0,82.0,78.9,3.00,15.00,12.00,200K,"Anthropic official, LMSYS Arena"
Qwen 3 235B,85.6,92.8,81.7,85.4,54.5,72.0,80.2,69.0,74.5,72.5,85.0,83.0,78.5,0.50,2.00,1.63,128K,"Alibaba official, open weights"
Google Gemini 2.0 Flash,80.9,92.0,75.5,87.5,75.1,70.0,78.5,70.2,73.5,73.0,80.0,83.0,77.8,0.15,0.60,0.49,1M,"Google official, cost comparison"
Mistral Large,84.5,92.7,83.1,89.7,58.3,53.2,82.0,70.5,75.3,73.8,85.5,85.0,77.2,3.00,9.00,7.50,128K,"Mistral official, benchmarks"
Qwen 3 32B,81.3,91.5,78.4,79.3,49.2,65.5,77.5,65.5,71.0,68.0,80.0,80.0,74.3,0.20,0.80,0.65,128K,"Alibaba official, community"
Meta Llama 4 Maverick,78.5,93.2,79.8,82.0,63.2,54.0,76.2,66.5,67.3,69.5,76.5,80.0,73.8,0.27,0.85,0.71,1M,"Meta official, open weights"
OpenAI GPT-4o,87.2,95.3,90.2,92.0,49.9,9.3,85.2,66.0,70.5,68.0,74.6,85.0,73.5,2.50,10.00,8.13,128K,"OpenAI official, LMSYS Arena"
Qwen 3 14B,78.8,90.3,75.2,76.2,46.5,58.0,74.8,62.0,68.5,65.0,77.0,78.0,70.5,0.10,0.40,0.33,128K,"Alibaba official, cost efficient"
Meta Llama 3.3 70B,79.3,95.1,73.2,81.7,48.5,21.3,73.8,64.3,62.7,64.5,69.5,76.0,70.0,0.25,0.70,0.59,128K,"Meta official, open weights"
Meta Llama 4 Scout,74.2,91.4,73.5,79.5,58.4,48.0,71.5,65.0,63.8,66.0,72.0,77.0,68.9,0.18,0.59,0.49,10M,"Meta official, long context"
OpenAI GPT-4,86.4,93.2,67.0,89.0,45.0,6.7,84.0,62.5,67.3,65.5,70.0,80.0,68.7,30.00,60.00,52.50,8K,"OpenAI official, legacy model"